{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "options = webdriver.ChromeOptions() \n",
    "options.add_argument('--headless') \n",
    "options.add_argument('--no-sandbox') \n",
    "options.add_argument('--disable-dev-shm-usage') \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import urllib.request\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_crawling(keyword,START_DATE,END_DATE):\n",
    "    origin_URL = (f'ttp://search.khan.co.kr/search.html?stb=khan&dm=5&d1={start_date}~{end_date}&q=') # start_date와 end_date넣어주어서 범위지정해줍니다.\n",
    "    URL = origin_URL + keyword + '&pg=' # 링크 만들어서 getlink로 보내줍니다.\n",
    "    get_link(URL)\n",
    "    \n",
    "def get_link(hURL):\n",
    "    firstURL = \"h\" + hURL # 하이퍼링크이슈가 있어서 'h' 제거후 붙인다음 마지막에 붙여주었습니다\n",
    "    driver = webdriver.Chrome(executable_path=r'C:/chromedriver/chromedriver.exe')\n",
    "    driver.implicitly_wait(3)\n",
    "    driver.get(firstURL)\n",
    "\n",
    "    firstsoup = BeautifulSoup(driver.page_source, 'html.parser') # 첫화면에서 soup따주고\n",
    "    num1 = firstsoup.select('#container > div.content > div.news.section > h3 > span')\n",
    "    num2 = num1[0].get_text()#날자 따준다음\n",
    "    num3 = num2.replace('(총 ', '')\n",
    "    num4 = num3.replace(' 건 검색)', '')\n",
    "    article_num = int(num4.replace(',', '')) # (총 xx,xxx 건 검색) 전처리 이후 int로 변환해줍니다.\n",
    "\n",
    "    page_num = int((article_num / 10) +1) # page수 구해준후\n",
    "    \n",
    "    for i in range(1,page_num+1):# for문에서 page수 만큼 반복시켜줍니다. \n",
    "        URL = firstURL + str(i)\n",
    "        driver.get(URL)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        for i in range(2,12): # 마지막 페이지에선 갯수가 모잘라서 에러가 납니다. 방지하기 위해 try, except문으로 마지막페이지 에러방지했습니다.\n",
    "            try:\n",
    "                link = soup.select(f'#container > div.content > div.news.section > dl:nth-child({i}) > dt > a')[0]['href'][:-12]\n",
    "                # 링크 따주고\n",
    "                LINK_OF_ARTICLE.append(link) # 어펜드 먼저 해줍니다.\n",
    "                \n",
    "            except IndexError as ERR:\n",
    "                pass\n",
    "\n",
    "\n",
    "            \n",
    "def get_text(link): #링크 받아서\n",
    "    req = urllib.request.Request(link, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'})        \n",
    "    source_code_from_url = urllib.request.urlopen(req)\n",
    "    articlesoup = BeautifulSoup(source_code_from_url, 'lxml', from_encoding='utf-8') # soup 따주고\n",
    "    \n",
    "    try:\n",
    "        title = articlesoup.select('#article_title')[0].get_text()\n",
    "        # title을 따주는데 가끔 에러가 발생합니다. (\"div.highlight\"가 있는경우) 워낙 많은양의 기사를 가져오다보니 과감히 배제하였습니다.\n",
    "    except:\n",
    "        title = \"ERROR\"\n",
    "   \n",
    "    try:\n",
    "        date = articlesoup.select('#container > div.art_header.borderless > div.function_wrap > div.pagecontrol > div > em:nth-child(1)')[0].get_text()[5:15]\n",
    "        # date도 마찬가지로 (\"div.highlight\"가 있는경우) 에러가 발생하는데, 극히 일부분이기에 과감히 배제하였습니다.\n",
    "        # 또한 ERROR 를 추가해 전처리 과정에서 행을 삭제하였습니다.\n",
    "    except:        \n",
    "        date = \"ERROR\"\n",
    "        \n",
    "    all_article = '' # 기사 본문 생성하고\n",
    "    for article in articlesoup.select('p.content_text'):\n",
    "        all_article += article.get_text() # 하나 하나 붙여줍니다.\n",
    "        all_article += '.' # 중간중간 '.'을 추가해 단어의 합체를 막았습니다.\n",
    "        # ex) 반도체 역할 톡톡히\n",
    "        #\n",
    "        #     삼성에서는 이러한~~              ->반도체 역할 톡톡히삼성에서는  ->  반도체 역할 톡톡히.삼성에서는\n",
    "        \n",
    "        \n",
    "    TITLE_OF_ARTICLE.append(title)\n",
    "    DATE_OF_ARTICLE.append(date)\n",
    "    CONTENT_OF_ARTICLES.append(all_article) #어펜드 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0번째 기사 완료\n",
      "100번째 기사 완료\n",
      "200번째 기사 완료\n",
      "300번째 기사 완료\n",
      "400번째 기사 완료\n",
      "500번째 기사 완료\n",
      "600번째 기사 완료\n",
      "700번째 기사 완료\n",
      "800번째 기사 완료\n",
      "900번째 기사 완료\n",
      "1000번째 기사 완료\n",
      "1100번째 기사 완료\n",
      "1200번째 기사 완료\n",
      "1300번째 기사 완료\n",
      "1400번째 기사 완료\n",
      "1500번째 기사 완료\n",
      "1600번째 기사 완료\n",
      "1700번째 기사 완료\n",
      "1800번째 기사 완료\n",
      "1900번째 기사 완료\n",
      "2000번째 기사 완료\n",
      "2100번째 기사 완료\n",
      "2200번째 기사 완료\n",
      "2300번째 기사 완료\n",
      "2400번째 기사 완료\n",
      "2500번째 기사 완료\n",
      "2600번째 기사 완료\n",
      "2700번째 기사 완료\n",
      "2800번째 기사 완료\n",
      "2900번째 기사 완료\n",
      "3000번째 기사 완료\n",
      "3100번째 기사 완료\n",
      "3200번째 기사 완료\n",
      "3300번째 기사 완료\n",
      "3400번째 기사 완료\n",
      "3500번째 기사 완료\n",
      "3600번째 기사 완료\n",
      "3700번째 기사 완료\n",
      "3800번째 기사 완료\n",
      "3900번째 기사 완료\n",
      "4000번째 기사 완료\n",
      "4100번째 기사 완료\n",
      "4200번째 기사 완료\n",
      "4300번째 기사 완료\n",
      "4400번째 기사 완료\n",
      "4500번째 기사 완료\n",
      "끝! 걸린시간 = 3075.575517654419초\n"
     ]
    }
   ],
   "source": [
    "start_date = '20100101'\n",
    "end_date = '20201231'\n",
    "\n",
    "keyword = '인공지능'\n",
    "TITLE_OF_ARTICLE = []\n",
    "DATE_OF_ARTICLE = []\n",
    "CONTENT_OF_ARTICLES = []\n",
    "LINK_OF_ARTICLE = []\n",
    "\n",
    "#전역변수 초기화 및 날짜, 키워드 설정\n",
    "\n",
    "starttime = time.time()\n",
    "start_crawling(keyword,start_date,end_date) #링크만 먼저 따줍니다.\n",
    "\n",
    "#이후 링크에서 링크를 하나씩 가져와서 get_text해줍니다.\n",
    "for i in range(0,len(LINK_OF_ARTICLE)):\n",
    "    link = LINK_OF_ARTICLE[i]\n",
    "    get_text(link)\n",
    "    \n",
    "    if i % 100 == 0: # 백번째 마다 완료문구 나오게 설정했습니다.\n",
    "        print(f'{i}번째 기사 완료')\n",
    "\n",
    "runtime = time.time() - starttime\n",
    "print(f'끝! 걸린시간 = {runtime}초')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(LINK_OF_ARTICLE, TITLE_OF_ARTICLE, CONTENT_OF_ARTICLES, DATE_OF_ARTICLE)), columns =['LINK','Title', 'Content', 'Date'])\n",
    "df = df[df.Title != \"ERROR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content'] = df['Content'].str.replace(r\"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)\" , '')\n",
    "df['Content'] = df['Content'].str.replace('[', '')\n",
    "df['Content'] = df['Content'].str.replace(']', '')\n",
    "df['Content'] = df['Content'].str.replace(\"'\", \"\")  \n",
    "df['Content'] = df['Content'].str.replace('\"', '')\n",
    "df['Content'] = df['Content'].str.replace(',', '')\n",
    "df['Content'] = df['Content'].str.replace(r'\\\\n', '') \n",
    "df['Content'] = df['Content'].str.replace(r'\\\\r', '')\n",
    "df['Content'] = df['Content'].str.replace(r'\\\\xa0', '')\n",
    "df['Content'] = df['Content'].str.replace('‘', '')\n",
    "df['Content'] = df['Content'].str.replace('’', '')\n",
    "df['Content'] = df['Content'].str.replace('“', '')\n",
    "df['Content'] = df['Content'].str.replace('”', '')\n",
    "df['Content'] = df['Content'].str.replace('//', '')\n",
    "df['Content'] = df['Content'].str.replace('ㆍ', '')\n",
    "df['Content'] = df['Content'].str.replace('  ', '')\n",
    "df['Content'] = df['Content'].str.replace('·', '')\n",
    "df['Content'] = df['Content'].str.replace('…', '')\n",
    "df['Content'] = df['Content'].str.replace('〈br〉', '')\n",
    "df['Content'] = df['Content'].str.replace('「', '')\n",
    "df['Content'] = df['Content'].str.replace('」', '')\n",
    "df['Content'] = df['Content'].str.replace(r'\\\\', '')\n",
    "df['Content'] = df['Content'].str.lstrip()\n",
    "df['Content'] = df['Content'].str.replace(r'\\(([^)]+)\\)','')\n",
    "\n",
    "df['Content'] = df['Content'].str.replace('[.][.]', '.')\n",
    "\n",
    "df['Title'] = df['Title'].str.replace(r'\\[([^)]+)\\]','')\n",
    "df['Title'] = df['Title'].str.replace('‘', '')\n",
    "df['Title'] = df['Title'].str.replace('’', '')\n",
    "df['Title'] = df['Title'].str.replace(',', '')\n",
    "df['Title'] = df['Title'].str.replace('[', '')\n",
    "df['Title'] = df['Title'].str.replace(']', '')\n",
    "df['Title'] = df['Title'].str.replace('…', '')\n",
    "df['Title'] = df['Title'].str.replace('-', '')\n",
    "df['Title'] = df['Title'].str.replace('·', '')\n",
    "df['Title'] = df['Title'].str.replace('“', '')\n",
    "df['Title'] = df['Title'].str.replace('”', '')\n",
    "df['Title'] = df['Title'].str.replace('\"', '')\n",
    "df['Title'] = df['Title'].str.replace('···', '')\n",
    "df['Title'] = df['Title'].str.replace(\"'\", '')\n",
    "df['Title'] = df['Title'].str.replace('?', '')\n",
    "df['Title'] = df['Title'].str.replace('\\n', '')\n",
    "\n",
    "#이후 전처리 과정을 한번 더 거칠꺼라 기본적인 부분만 전처리 하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'경향신문_{keyword}({start_date}~{end_date}).csv', encoding = 'utf-8-sig', index_label = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
