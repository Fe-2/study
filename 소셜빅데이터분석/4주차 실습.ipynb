{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "options = webdriver.ChromeOptions() \n",
    "options.add_argument('--headless') \n",
    "options.add_argument('--no-sandbox') \n",
    "options.add_argument('--disable-dev-shm-usage') \n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.parse import quote\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_URL = 'https://news.naver.com/main/ranking/popularMemo.nhn?date=20210308'\n",
    "\n",
    "TITLE_OF_ARTICLE = []\n",
    "OFFICE_OF_ARTICLE = []\n",
    "URL_OF_ARTICLE = []\n",
    "DATE_OF_ARTICLE = []\n",
    "CONTENT_OF_ARTICLE = []\n",
    "COMMENT_OF_ARTICLE = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_from_news_title(URL):\n",
    "    req = urllib.request.Request(URL, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    source_code_from_url = urllib.request.urlopen(req)\n",
    "    soup = BeautifulSoup(source_code_from_url, 'lxml', from_encoding='utf-8')\n",
    "    \n",
    "    for title in soup.select('div.list_content'):\n",
    "        url_link = title.select('a')\n",
    "        article_URL = 'https://news.naver.com' + url_link[0]['href']\n",
    "        URL_OF_ARTICLE.append(article_URL)\n",
    "\n",
    "        position = article_URL.index('read.nhn?')\n",
    "        comment_URL = article_URL[:position+9] + 'm_view=1&includeAllCount=true&' + article_URL[position+9:] \n",
    "\n",
    "        get_text(article_URL) \n",
    "        get_comment(comment_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(article_URL):\n",
    "    req = urllib.request.Request(article_URL, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    source_code_from_url = urllib.request.urlopen(req)\n",
    "    soup = BeautifulSoup(source_code_from_url, 'lxml', from_encoding='utf-8')\n",
    "\n",
    "    #office_link = soup.select('head > meta:nth-of-type(6)')\n",
    "    office_link = soup.select('div.press_logo > a > img')\n",
    "    OFFICE_OF_ARTICLE.append(office_link[0]['title'])\n",
    "\n",
    "    title_link = soup.select('div.article_info > h3')\n",
    "    TITLE_OF_ARTICLE.append(title_link[0].string)\n",
    "\n",
    "    date_of_article = soup.select('span.t11')\n",
    "    DATE_OF_ARTICLE.append(date_of_article[0].string)\n",
    "\n",
    "    string_item = '' #빈 문자열 만드는 것\n",
    "    for item in soup.select('div._article_body_contents'):  \n",
    "        string = str(item.find_all(text=True))\n",
    "        string_item += string \n",
    "    CONTENT_OF_ARTICLE.append(string_item) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment(comment_URL):\n",
    "    driver = webdriver.Chrome(executable_path=r'C:/chromedriver/chromedriver.exe')\n",
    "    driver.implicitly_wait(5)\n",
    "    driver.get(comment_URL) #URL 창을 여는 것\n",
    "\n",
    "    \n",
    "    contents = driver.find_elements_by_css_selector('span.u_cbox_contents')\n",
    "    comments = []\n",
    "    for comment in contents:\n",
    "        comments.append(comment.text) #BS4에서는 .string이었고 셀 에서는 .text\n",
    "\n",
    "    COMMENT_OF_ARTICLE.append(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 1640.3320517539978\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "get_link_from_news_title(TARGET_URL)\n",
    "print(\"time :\", time.time() - start)\n",
    "\n",
    "df = pd.DataFrame(list(zip(DATE_OF_ARTICLE, TITLE_OF_ARTICLE, OFFICE_OF_ARTICLE, CONTENT_OF_ARTICLE, URL_OF_ARTICLE, COMMENT_OF_ARTICLE)), columns =['Date', 'Title', 'Office', 'Content', 'URL', 'Comment']) \n",
    "\n",
    "df.to_csv(\"4주차실습(네이버).csv\",index=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_URL = 'https://www.chosun.com/search/query=%EC%9D%B8%EA%B6%8C&siteid=www&sort=1&date_period=1w&writer=&field=&emd_word=&expt_word=&opt_chk=false/'\n",
    "\n",
    "\n",
    "TITLE_OF_ARTICLE = []\n",
    "DATE_OF_ARTICLE = []\n",
    "CONTENT_OF_ARTICLE = []\n",
    "URL_OF_ARTICLE = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_from_news_title(URL): \n",
    "    driver = webdriver.Chrome(executable_path=r'C:/chromedriver/chromedriver.exe')\n",
    "    driver.implicitly_wait(5)\n",
    "    driver.get(URL)\n",
    "\n",
    "    while True: # 반복문 break를 만나기 전까지 무한히 반복한다.\n",
    "        try: # 더보기 버튼을 더 이상 누를수 없을 때 까지 반복한다.\n",
    "            more_button = driver.find_element_by_xpath('//*[@id=\"load-more-stories\"]') # 더보기 버튼 위치\n",
    "            more_button.click() # 더보기 버튼 클릭\n",
    "            time.sleep(1) # 페이지가 로딩되는데 시간이 필요하므로 1초의 텀을 준다.\n",
    "        except: # 더보기 버튼을 더 이상 누를수 없을 때 실행된다.\n",
    "            break # 반복문 종료\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser') # soup로 html 소스를 가져온다.\n",
    "    \n",
    "    for title in soup.select('div.story-card__headline-container.\\|.box--margin-bottom-xs > h5'): # 기사들의 URL이 있는 링크에 접근해서 하나씩 title로 넘겨주는 반복문...h3이 나올수도 있음\n",
    "        title_link = title.select('span')\n",
    "        TITLE_OF_ARTICLE.append(title_link[0].get_text())\n",
    "        #TITLE_OF_ARTICLE.append(title.span.text) \n",
    "\n",
    "        article_link = title.select('a') # URL이 위치하는 태그를 가리킨다.\n",
    "        article_URL = 'https://www.chosun.com' + article_link[0]['href'] # URL 리스트에 저장한다.\n",
    "        \n",
    "        get_text(article_URL) # URL을 get_text 함수에 넘겨준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(URL):\n",
    "    driver = webdriver.Chrome(executable_path=r'C:/chromedriver/chromedriver.exe')\n",
    "    driver.implicitly_wait(5)\n",
    "    driver.get(URL)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    date_of_article = soup.select('div.article-dateline.\\|.flex.flex--justify-space-between.flex--align-items-center.box--border.box--border-grey-40.box--border-horizontal.box--border-horizontal-bottom.box--pad-bottom-sm > span')\n",
    "    DATE_OF_ARTICLE.append(date_of_article[0].string)\n",
    "\n",
    "    content_of_article = soup.select('section.article-body')\n",
    "    string_item = str(content_of_article[0].find_all(text=True))\n",
    "    CONTENT_OF_ARTICLE.append(string_item) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_link_from_news_title(TARGET_URL)\n",
    "\n",
    "df = pd.DataFrame(list(zip(TITLE_OF_ARTICLE, CONTENT_OF_ARTICLE, DATE_OF_ARTICLE)), columns =['Title', 'Content', 'Date']) \n",
    "\n",
    "df.to_csv(\"4주차실습(조선일보).csv\",index=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
